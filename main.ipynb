{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keystroke Dynamics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some explanation here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keylogger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a keyloger for the first step of the project, the Data Acquisition. In order to extrapolate useful results and metrics that we can use in for further analysis, two keyloggers need to run simultaniously: one that logs every time a button is pressed, and another that logs every time a button is released. Our keylogger saves separate files for pressed and released buttons. We save a new file every 10 seconds with the keys pressed/released during that interval, over a timespan of 1 minute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import keyboard # for keylogs\n",
    "from threading import Timer\n",
    "from datetime import datetime\n",
    "import time\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logging parameters can be easily modified. In the current configuration, each keylogger saves a file every 10 seconds, over a total of 60 seconds. In the end, 12 files will be saved in two folders: 6 for pressed keys, 6 for released keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logging parameters\n",
    "SEND_REPORT_EVERY = 10 # in seconds, 60 means 1 minute and so on\n",
    "INTERRUPT_AFTER = 60 # in seconds, 60 means 1 minute and so on\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The keylogger works by triggering an event each time a button on the keyboard is pressed or released. The key is properly formatted with a timestamp of when it was pressed, and saved to a file. At the end of the SEND_REPORT_EVERY interval, the running text file is saved to the proper directory, and a new file is initialized. The keylogger exits at the end of the INTERRUPT_AFTER interval. When initializing the keylogger, the event type needs to be specified: \"press\" or \"release\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keylogger\n",
    "class Keylogger:\n",
    "    def __init__(self, interval, report_method=\"file\"):\n",
    "        # we gonna pass SEND_REPORT_EVERY to interval\n",
    "        self.interval = interval\n",
    "        self.report_method = report_method\n",
    "        # this is the string variable that contains the log of all \n",
    "        # the keystrokes within `self.interval`\n",
    "        self.log = \"\"\n",
    "        # record start & end datetimes\n",
    "        self.start_dt = datetime.now()\n",
    "        self.end_dt = datetime.now()\n",
    "        \n",
    "        self.event_type = \"\"\n",
    "        self.user = \"\"\n",
    "\n",
    "    def callback(self, event):\n",
    "        \"\"\"\n",
    "        This callback is invoked whenever a keyboard event is occured\n",
    "        (i.e when a key is released in this example)\n",
    "        \"\"\"\n",
    "        name = event.name\n",
    "        ts = time.time()\n",
    "        timestamp = datetime.fromtimestamp(ts).strftime('%Y-%m-%d %H:%M:%S %f')\n",
    "        if len(name) > 1:\n",
    "            # not a character, special key (e.g ctrl, alt, etc.)\n",
    "            # uppercase with []\n",
    "            if name == \"space\":\n",
    "                # \" \" instead of \"space\"\n",
    "                name = \"[space]\"\n",
    "            elif name == \"enter\":\n",
    "                # add a new line whenever an ENTER is pressed\n",
    "                name = \"[enter]\"\n",
    "            elif name == \"decimal\":\n",
    "                name = \".\"\n",
    "            else:\n",
    "                # replace spaces with underscores\n",
    "                name = name.replace(\" \", \"_\")\n",
    "                name = f\"[{name.upper()}]\"\n",
    "        # finally, add the key name to our global `self.log` variable\n",
    "        self.log += \"{} - {}\\n\".format(timestamp, name)\n",
    "        # exit code if timer expired\n",
    "        if time.time() - start_time > INTERRUPT_AFTER:\n",
    "            sys.exit()\n",
    "\n",
    "    def update_filename(self):\n",
    "        # construct the filename to be identified by start & end datetimes\n",
    "        start_dt_str = str(self.start_dt)[:-7].replace(\" \", \"-\").replace(\":\", \"\")\n",
    "        end_dt_str = str(self.end_dt)[:-7].replace(\" \", \"-\").replace(\":\", \"\")\n",
    "        self.filename = f\"logs/{self.user}/{self.event_type}/keylog-{start_dt_str}_{end_dt_str}\"\n",
    "\n",
    "    def report_to_file(self):\n",
    "        \"\"\"This method creates a log file in the current directory that contains\n",
    "        the current keylogs in the `self.log` variable\"\"\"\n",
    "        # open the file in write mode (create it)\n",
    "        with open(f\"{self.filename}.txt\", \"w\") as f:\n",
    "            # write the keylogs to the file\n",
    "            print(self.log, file=f)\n",
    "        print(f\"[+] Saved {self.filename}.txt\")\n",
    "\n",
    "    def report(self):\n",
    "        \"\"\"\n",
    "        This function gets called every `self.interval`\n",
    "        It basically sends keylogs and resets `self.log` variable\n",
    "        \"\"\"\n",
    "        if self.log:\n",
    "            # if there is something in log, report it\n",
    "            self.end_dt = datetime.now()\n",
    "            # update `self.filename`\n",
    "            self.update_filename()\n",
    "            if self.report_method == \"file\":\n",
    "                self.report_to_file()\n",
    "            # if you want to print in the console, uncomment below line\n",
    "            # print(f\"[{self.filename}] - {self.log}\")\n",
    "            self.start_dt = datetime.now()\n",
    "        self.log = \"\"\n",
    "        timer = Timer(interval=self.interval, function=self.report)\n",
    "        # set the thread as daemon (dies when main thread die)\n",
    "        timer.daemon = True\n",
    "        # start the timer\n",
    "        timer.start()\n",
    "\n",
    "    def start(self, user, event):\n",
    "        # record the start datetime\n",
    "        self.start_dt = datetime.now()\n",
    "        self.event_type = event\n",
    "        self.user = user\n",
    "        # start the keylogger\n",
    "        if event == 'press':\n",
    "            keyboard.on_press(callback=self.callback)\n",
    "        if event == 'release':\n",
    "            keyboard.on_release(callback=self.callback)\n",
    "        # start reporting the keylogs\n",
    "        self.report()\n",
    "        # block the current thread, wait until CTRL+C is pressed\n",
    "        keyboard.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calling the keylogger\n",
    "# user = \"intruder\"\n",
    "# keylogger = Keylogger(interval=SEND_REPORT_EVERY, report_method=\"file\")\n",
    "# keylogger.start(user, \"press\")\n",
    "# keylogger.start(user, \"release\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the logs for known users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step of our study is the preprocessing of data so we can easily study and exploit it for our further research. We need to take the textual data from the keylogger and transform it into data structures that are more efficient and usefult for the analysis that we plan to do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import glob\n",
    "import pandas as pd\n",
    "import sys\n",
    "import math\n",
    "\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hold_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.1491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.1069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.1169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.1417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.1146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224395</th>\n",
       "      <td>0.0699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224396</th>\n",
       "      <td>0.0969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224397</th>\n",
       "      <td>0.0790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224398</th>\n",
       "      <td>0.0807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224399</th>\n",
       "      <td>0.1018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>224400 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        hold_time\n",
       "0          0.1491\n",
       "1          0.1069\n",
       "2          0.1169\n",
       "3          0.1417\n",
       "4          0.1146\n",
       "...           ...\n",
       "224395     0.0699\n",
       "224396     0.0969\n",
       "224397     0.0790\n",
       "224398     0.0807\n",
       "224399     0.1018\n",
       "\n",
       "[224400 rows x 1 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_dataframe = pd.DataFrame()\n",
    "data = pd.read_csv('datasets/benchmark/DSL-StrongPasswordData.csv')\n",
    "\n",
    "data_hold = data.filter(regex=\"H.\")\n",
    "merged_dataframe['hold_time'] = data_hold.stack().reset_index()[0]\n",
    "\n",
    "data_press_press = data.filter=(regex=\"DD.\")\n",
    "merged_dataframe['press_press_time'] = data_press_press.stack().reset_index()[0]\n",
    "\n",
    "data_press_release = data.filter=(regex=\"UD.\")\n",
    "merged_dataframe['press_release_time'] = data_press_release.stack().reset_index()[0]\n",
    "\n",
    "merged_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firtly, for each file in the key pressed and key released directories, we need to convert simple textual data into a useful data structure that is easy to manipulate. For our purposes, the ideal data structure is a DataFrame from Pandas.\n",
    "A DataFrame is two-dimensional, size-mutable, potentially heterogeneous tabular data. This data structure also contains labeled axes (rows and columns). Arithmetic operations align on both row and column labels. It represents the primary pandas data structure.\n",
    "\n",
    "Once out pressed and released data is properly formatted, we can merge them into a single file, detailing for each key when it was pressed and released in a single dataset. In order to do that firstly we remove any duplicated that might have occured when saving the separate text files. Then, for each timestamp of a released key, we search for the nearest timestamp for which the same key was pressed. We can then take that data and save it in a single dataset. We also identify in this dataset the name of the user who created this data, in order to properly differentiate between user datasets in our analysis.\n",
    "\n",
    "Finally we measure a few key metrics and add them to our final dataset:\n",
    "- press_press_time: a list of time intervals between different subsequent key presses.\n",
    "- release_release_time: a list of time intervals between subsequent key releases.\n",
    "- press_release_time: a list of time intervals that represent the time interval for which a key is released and the next key is pressed.\n",
    "- hold_time: a list of time intervals that represent the time for which a certain key is pressed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(user=None):\n",
    "\n",
    "    # Key-based\n",
    "\n",
    "    press_files = glob.glob(f\"logs/{user}/press/*.txt\")\n",
    "    press_rows = []\n",
    "\n",
    "    release_files = glob.glob(f\"logs/{user}/release/*.txt\")\n",
    "    release_rows = []\n",
    "\n",
    "    for press_file in press_files:\n",
    "        with open(press_file) as fp:\n",
    "            line = fp.readline()\n",
    "            while line:\n",
    "                if line == \"\\n\":\n",
    "                    line = fp.readline()\n",
    "                    continue\n",
    "                timestamp = datetime.strptime(\n",
    "                    line.strip().split(\" - \")[0], '%Y-%m-%d %H:%M:%S %f')\n",
    "                key = line.strip().split(\" - \")[1].lower()\n",
    "                press_rows.append({\n",
    "                    \"timestamp\": timestamp,\n",
    "                    \"key\": key\n",
    "                })\n",
    "                line = fp.readline()\n",
    "\n",
    "    for release_file in release_files:\n",
    "        with open(release_file) as fp:\n",
    "            line = fp.readline()\n",
    "            while line:\n",
    "                if line == \"\\n\":\n",
    "                    line = fp.readline()\n",
    "                    continue\n",
    "                timestamp = datetime.strptime(\n",
    "                    line.strip().split(\" - \")[0], '%Y-%m-%d %H:%M:%S %f')\n",
    "                key = line.strip().split(\" - \")[1].lower()\n",
    "                release_rows.append({\n",
    "                    \"timestamp\": timestamp,\n",
    "                    \"key\": key\n",
    "                })\n",
    "                line = fp.readline()\n",
    "\n",
    "    press_df = pd.DataFrame(press_rows)\n",
    "    release_df = pd.DataFrame(release_rows)\n",
    "\n",
    "    # Map release -> press and build dataset\n",
    "    dataset_rows = []\n",
    "    for _, row in release_df.drop_duplicates(subset=['key']).iterrows():\n",
    "        sub_df = release_df[release_df[\"key\"] == row['key']]\n",
    "        lower_bound = None\n",
    "        for sub_id, sub_row in sub_df.reset_index().iterrows():\n",
    "            higher_bound = sub_row[\"timestamp\"]\n",
    "            if sub_id == 0:\n",
    "                press_time = press_df[(press_df[\"key\"] == sub_row[\"key\"]) & (\n",
    "                    press_df[\"timestamp\"] < higher_bound)].iloc[0][\"timestamp\"]\n",
    "            else:\n",
    "                press_time = press_df[(press_df[\"key\"] == sub_row[\"key\"]) & (\n",
    "                    press_df[\"timestamp\"] < higher_bound) & (press_df[\"timestamp\"] > lower_bound)].iloc[0][\"timestamp\"]\n",
    "            lower_bound = higher_bound\n",
    "            dataset_rows.append({\n",
    "                \"key\": sub_row[\"key\"],\n",
    "                \"pressed\": press_time,\n",
    "                \"released\": sub_row[\"timestamp\"],\n",
    "                \"user\": user\n",
    "            })\n",
    "\n",
    "    dataset = pd.DataFrame(dataset_rows).sort_values(['pressed'])\n",
    "\n",
    "    dataset[\"hold_time\"] = (dataset[\"released\"] -\n",
    "                            dataset[\"pressed\"]) / pd.Timedelta(microseconds=1)\n",
    "    press_press_time = []\n",
    "    press_release_time = []\n",
    "    release_release_time = []\n",
    "\n",
    "    for index, row in dataset.reset_index().iterrows():\n",
    "        press_press_delta = (dataset.iloc[index+1][\"pressed\"] -\n",
    "                             dataset.iloc[index][\"pressed\"]) / pd.Timedelta(microseconds=1)\n",
    "        press_press_time.append(press_press_delta)\n",
    "        release_release_delta = (\n",
    "            dataset.iloc[index+1][\"released\"] - dataset.iloc[index][\"released\"]) / pd.Timedelta(microseconds=1)\n",
    "        release_release_time.append(release_release_delta)\n",
    "        press_release_delta = (\n",
    "            dataset.iloc[index+1][\"pressed\"] - dataset.iloc[index][\"released\"]) / pd.Timedelta(microseconds=1)\n",
    "        press_release_time.append(press_release_delta)\n",
    "\n",
    "        if index == len(dataset) - 2:\n",
    "            press_press_time.append(0)\n",
    "            press_release_time.append(0)\n",
    "            release_release_time.append(0)\n",
    "            break\n",
    "\n",
    "    dataset[\"press_press_time\"] = press_press_time\n",
    "    dataset[\"press_release_time\"] = press_release_time\n",
    "    dataset[\"release_release_time\"] = release_release_time\n",
    "\n",
    "    kb_dataset = dataset.copy().reset_index().drop(columns=['index'])\n",
    "    kb_dataset.to_csv(f\"datasets/key_based/{user}_data.csv\", index=False)\n",
    "\n",
    "    # Word-based\n",
    "    wb_dataset = dataset.copy().reset_index().drop(columns=['index'])\n",
    "    wb_dataset[\"word_number\"] = 0\n",
    "    word_number = 0\n",
    "    for index, row in wb_dataset.iterrows():\n",
    "        wb_dataset.loc[index, \"word_number\"] = word_number\n",
    "        if row['key'] == \"[space]\":\n",
    "            word_number += 1\n",
    "\n",
    "    wb_dataset = wb_dataset.groupby(['word_number', 'user']).mean().reset_index().drop(columns=['word_number'])\n",
    "    \n",
    "    wb_dataset.to_csv(f\"datasets/word_based/{user}_data.csv\", index=False)\n",
    "\n",
    "    return kb_dataset, wb_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "registered_users = [\"alex\", \"stefan\", \"zihao\", \"zineb\"]\n",
    "for user in registered_users:\n",
    "    preprocess(user)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part of our code represents the second main task of the project: Data Exploration. We compute multiple different statistics using the generated and processed datasets. For our experimentation, each member of our team typed the same text using the keylogger, and a dataset was generated for each member. We then use them to compare different metrics and explore similarities and differences, as well as which metrics can help us identify a user just using keystroke dynamics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import glob\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compute several main statistics to start with, connected to our main metrics in the dataset:\n",
    "- average_hold_time: average hold time\n",
    "- average_press_press_time: average time between presses\n",
    "- average_release_release_time: average time between releases\n",
    "- average_press_release_time: average time between press and release of different keys\n",
    "- apm: number of key presses per minute\n",
    "- error_rate: error rate percentage (number of times backspace is pressed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical analysis\n",
    "def additional_stats(dataset):\n",
    "    end_time = datetime.strptime(dataset.iloc[len(\n",
    "        dataset)-1][\"pressed\"],'%Y-%m-%d %H:%M:%S.%f')\n",
    "    start_time = datetime.strptime(dataset.iloc[0][\"pressed\"],'%Y-%m-%d %H:%M:%S.%f')\n",
    "    apm = len(dataset) * 60 * math.pow(10, 6) / ((end_time - start_time) / pd.Timedelta(microseconds=1))\n",
    "    error_rate = len(\n",
    "        dataset[dataset[\"key\"] == \"[backspace]\"]) * 100 / len(dataset)\n",
    "    \n",
    "    return {\n",
    "        \"apm\":apm,\n",
    "        \"error_rate\": error_rate\n",
    "    }\n",
    "    \n",
    "def compute_statistics(dataset, group):\n",
    "    average_hold_time = dataset[\"hold_time\"].mean()\n",
    "    average_press_press_time = dataset.iloc[:len(\n",
    "        dataset)-1][\"press_press_time\"].mean()\n",
    "    average_press_release_time = dataset.iloc[:len(\n",
    "        dataset)-1][\"press_release_time\"].mean()\n",
    "    average_release_release_time = dataset.iloc[:len(\n",
    "        dataset)-1][\"release_release_time\"].mean()\n",
    "    ret = {\n",
    "            \"average_hold_time\": average_hold_time,\n",
    "            \"average_press_press_time\": average_press_press_time,\n",
    "            \"average_press_release_time\": average_press_release_time,\n",
    "            \"average_release_release_time\": average_release_release_time,\n",
    "            \"user\": dataset['user'][0]\n",
    "        }\n",
    "    if group == \"key\":\n",
    "        add_stats = additional_stats(dataset)\n",
    "        ret['apm'] = add_stats['apm']\n",
    "        ret['error_rate'] = add_stats['error_rate']\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# storing the statistics\n",
    "def loadData(group=\"key\"):\n",
    "    files = glob.glob(f\"datasets/{group}_based/*.csv\")\n",
    "    rows = []\n",
    "    dataframe = pd.DataFrame()\n",
    "    for file in files:\n",
    "        dataset = pd.read_csv(file)\n",
    "        dataframe = dataframe.append(dataset)\n",
    "        # Analyse statistique\n",
    "        stats = compute_statistics(dataset, group)\n",
    "        if group == \"key\":\n",
    "            stats[\"user\"] = dataset.iloc[0][\"user\"]\n",
    "        rows.append(stats)\n",
    "    return dataframe, pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_word, statistics_word = loadData(\"word\")\n",
    "dataframe, statistics = loadData(\"key\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistics visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pylab import rcParams\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Table view of the key statistical metrics for each user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statistics_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BoxPlots for each metric for each user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def boxPlotStats(dataframe):    \n",
    "    features = ['hold_time', 'press_press_time', 'press_release_time', 'release_release_time']\n",
    "    fig, axs = plt.subplots(8, 2, figsize=(20, 10))\n",
    "    fig.subplots_adjust(top=2, hspace=0.4)\n",
    "    n=0\n",
    "    for feature in features:\n",
    "        for user in registered_users:\n",
    "            axs[int(n/2), int(n%2)].text(.1,.9,user, horizontalalignment='left',transform=axs[int(n/2), int(n%2)].transAxes)\n",
    "            axs[int(n/2), int(n%2)].boxplot(dataframe[dataframe['user']==user][feature])\n",
    "            axs[int(n/2), int(n%2)].set_title(f\"{feature} (microseconds)\")\n",
    "            n+=1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boxPlotStats(dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boxPlotStats(dataframe_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the distribution of different metrics over time for each user. The hold_time duration gives us interesting results, with distinct distibutions of durations for each users, which can help us separate when identifying the keystrokes of a certain user. The press_time metric is less interesting for example, with a fairly similar distribution for different users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def barchartStats(dataframe):\n",
    "    merged = dataframe.copy()\n",
    "\n",
    "    # Rounding to closest milisecond\n",
    "    for feature in features:\n",
    "        merged[feature] = merged[feature].apply(lambda x: round(x/1000))\n",
    "    fig, axs = plt.subplots(8, 2, figsize=(20, 10))\n",
    "    fig.subplots_adjust(top=2, hspace=0.4)\n",
    "    n=0\n",
    "    for feature in features:\n",
    "        for user in registered_users:\n",
    "            mm = pd.DataFrame(merged[merged['user']==user].groupby(feature)[feature].count()).rename(columns={feature:'count'}).reset_index()\n",
    "            axs[int(n/2), int(n%2)].bar(mm[feature], mm['count'])\n",
    "            axs[int(n/2), int(n%2)].set_ylabel('count')\n",
    "            axs[int(n/2), int(n%2)].set_xlabel('duration (ms)')\n",
    "            axs[int(n/2), int(n%2)].set_title(f\"{feature} - {user}\")\n",
    "            n+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "barchartStats(dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "barchartStats(dataframe_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Heatmap showing the correlation between different metrics. We can conclude that there is a big contrast between the hold_time and press_release_time, which means that there metrics can be contrasting and deciding factors to study when identifying a user. Metrics with a higher degree of correlation give us less information because of the similarity, such as the correlation between press_release_time and release_release_time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation\n",
    "def correlationPlot(dataframe):\n",
    "    rcParams['figure.figsize'] = 10, 10\n",
    "    fig = plt.figure()\n",
    "    sns.heatmap(dataframe.corr(), annot=True, fmt=\".2f\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlationPlot(dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlationPlot(dataframe_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this graphic, for each metric and each user we calculate the value densities. This allows us to compare the typing styles between the different users efficiently, as we can see in the differences in densities for each user. This can allow us to study the possibility of identifying a user using keystroke dynamics. The hold_time density is especially interesting, with peaks that are significant in density and different for each user, making identification and classification easier to separate users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def densityChartPlot(dataframe):    \n",
    "    rcParams['figure.figsize'] = 15, 20\n",
    "\n",
    "    # separate data based on users\n",
    "    outcome_alex = dataframe[dataframe['user']=='alex'][features]\n",
    "    outcome_stefan = dataframe[dataframe['user']=='stefan'][features]\n",
    "    outcome_zihao = dataframe[dataframe['user']=='zihao'][features]\n",
    "    outcome_zineb = dataframe[dataframe['user']=='zineb'][features]\n",
    "\n",
    "    # init figure\n",
    "    fig, axs = plt.subplots(4, 1)\n",
    "    fig.suptitle('Features densities for users alex, stefan, zihao, zineb')\n",
    "    plt.subplots_adjust(left = 0.25, right = 0.9, bottom = 0.1, top = 0.95,\n",
    "                        wspace = 0.2, hspace = 0.4)\n",
    "\n",
    "    colors = ['blue', 'red', 'orange', 'pink']\n",
    "    # plot densities for outcomes\n",
    "    i = 0\n",
    "    for feature in features: \n",
    "        ax = axs[i]\n",
    "        outcome_alex[feature].plot(kind='density', ax=ax, subplots=True, \n",
    "                                    sharex=False, color=colors[0], legend=True,\n",
    "                                    label=feature + ' for User = Alex')\n",
    "        outcome_stefan[feature].plot(kind='density', ax=ax, subplots=True, \n",
    "                                    sharex=False, color=colors[1], legend=True,\n",
    "                                    label=feature + ' for User = Stefan')\n",
    "        outcome_zihao[feature].plot(kind='density', ax=ax, subplots=True, \n",
    "                                    sharex=False, color=colors[2], legend=True,\n",
    "                                    label=feature + ' for User = Zihao')\n",
    "        outcome_zineb[feature].plot(kind='density', ax=ax, subplots=True, \n",
    "                                    sharex=False, color=colors[3], legend=True,\n",
    "                                    label=feature + ' for User = Zineb')\n",
    "        ax.set_xlabel(feature + ' values')\n",
    "        ax.set_title(feature + ' density')\n",
    "        ax.grid('on')\n",
    "        ax.set_xlim(-600000, 600000)\n",
    "        i+=1\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "densityChartPlot(dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "densityChartPlot(dataframe_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some explanation here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clusterVisualisation(dataframe):\n",
    "    fig, axs = plt.subplots(3, 2, figsize=(20, 10))\n",
    "    fig.subplots_adjust(top=2, hspace=0.4)\n",
    "    n=0\n",
    "    for i in range(len(features)):\n",
    "        for j in range(i+1, len(features)):\n",
    "            values = dataframe[[features[i], features[j], 'user']]\n",
    "            col = values['user'].map({\n",
    "                'alex': 'blue',\n",
    "                'stefan': 'red',\n",
    "                'zihao': 'orange',\n",
    "                'zineb': 'pink'\n",
    "            })\n",
    "            values.plot.scatter(x=features[i], y=features[j], c=col, ax=axs[int(n/2), int(n%2)])\n",
    "            axs[int(n/2), int(n%2)].set_ylabel(features[j])\n",
    "            axs[int(n/2), int(n%2)].set_xlabel(features[i])\n",
    "            n+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterVisualisation(dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterVisualisation(dataframe_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.manifold import Isomap\n",
    "\n",
    "import pandas as pd\n",
    "import glob\n",
    "import math\n",
    "import pickle\n",
    "import json\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The k-nearest neighbors algorithm (k-NN) is a non-parametric classification method. It is used for classification and regression. In our case, we use it for classification. The input consists of the k closest training examples in a data set, while the output is a class membership. An object is classified by a plurality vote of its neighbors, with the object being assigned to the class most common among its k nearest neighbors (k is a positive integer, typically small). If k = 1, then the object is simply assigned to the class of that single nearest neighbor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildAndTrainKNN(dataframe, k, group=\"key\"):\n",
    "    model_dataframe = dataframe.copy()\n",
    "    X = model_dataframe[model_dataframe.columns.difference(\n",
    "            ['user', 'pressed', 'released', 'key'])]\n",
    "\n",
    "    lb_make = LabelEncoder()\n",
    "    model_dataframe['user_code'] = lb_make.fit_transform(model_dataframe[\"user\"])\n",
    "\n",
    "    y = model_dataframe['user_code'].values\n",
    "\n",
    "    model_dataframe = model_dataframe.drop(columns=['user'])\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(X_train)\n",
    "    X_train = scaler.transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    scalerPickle = open(f'models/scalerpickle_file_{group}', 'wb')\n",
    "    pickle.dump(scaler, scalerPickle)\n",
    "    \n",
    "    # Determine k\n",
    "    error_rates = []\n",
    "    for i in np.arange(1, 20):\n",
    "        new_model = KNeighborsClassifier(n_neighbors = i)\n",
    "        new_model.fit(X_train, y_train)\n",
    "        new_predictions = new_model.predict(X_test)\n",
    "        error_rates.append(np.mean(new_predictions != y_test))\n",
    "\n",
    "    plt.plot(error_rates)\n",
    "    \n",
    "    # Instantiate the model with 4 neighbors.\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    # Fit the model on the training data.\n",
    "\n",
    "    knn.fit(X_train, y_train)\n",
    "\n",
    "    knnPickle = open(f'models/knnpickle_file_{group}', 'wb')\n",
    "    pickle.dump(knn, knnPickle)\n",
    "\n",
    "    print(knn.score(X_test, y_test))\n",
    "    return X_train, y_train, X_test, y_test, knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test, knn = buildAndTrainKNN(dataframe, 8, \"key\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_word, y_train_word, X_test_word, y_test_word, knn_word = buildAndTrainKNN(dataframe_word, 9, \"word\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we determine K?\n",
    "With a given K value, we determine the boundaries of each class. We determine the proper K to use by looking at the error rates produced by using different values of K. We then choose the value of K that producec the lowest error rate. In our case, that value is around the 7.5 mark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The score output by the KNN algorithm represents the mean accuracy on a test dataset and given labels. We managed to achieve an accuracy rate of around 66.8%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classificationReportKNN(knn, X_test, y_test):\n",
    "    predictions = knn.predict(X_test)\n",
    "    print(classification_report(y_test, predictions))\n",
    "    print(\"----- Confusion Matrix ------\")\n",
    "    print(confusion_matrix(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classificationReportKNN(knn, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classificationReportKNN(knn_word, X_test_word, y_test_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient boosting is a type of machine learning boosting. It relies on the intuition that the best possible next model, when combined with previous models, minimizes the overall prediction error. The key idea is to set the target outcomes for this next model in order to minimize the error. Gradient boosting is used to minimize bias errors in the model. The cost function when this algorithm is used for classification is Log loss.\n",
    "\n",
    "Gradient Boosting has allowed us to increase dramatically the accuracy of our model, going up to 83% accuracy of prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientBoosting(X_train, y_train, X_test, y_test, group=\"key\"):   \n",
    "    gb = GradientBoostingClassifier(\n",
    "            n_estimators=100, learning_rate=1.0, max_depth=1, random_state=0).fit(X_train, y_train)\n",
    "    print(gb.score(X_test, y_test))\n",
    "    gbPickle = open('models/gbpickle_file', 'wb')\n",
    "    pickle.dump(gb, gbPickle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradientBoosting(X_train, y_train, X_test, y_test, \"key\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradientBoosting(X_train_word, y_train_word, X_test_word, y_test_word, \"word\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We applied our models to a practical test to determine their efficiency and accuracy in detecting a user based on their typing. We load a model, and a file taken from our keylogger, and test to see which user the model will predict."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The KNN model managed to predict Zineb's keystroke patterns with 85% of accuracy. On the below graph we see the percentages for all users. There is a significant margin of difference between the correct answer and the other users, which means that this model is accurate in correctly predicting a user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "loaded_knn = pickle.load(open('models/knnpickle_file', 'rb'))\n",
    "loaded_gb = pickle.load(open('models/gbpickle_file', 'rb'))\n",
    "loaded_scaler = pickle.load(open('models/scalerpickle_file', 'rb'))\n",
    "\n",
    "# Load file\n",
    "dataframe_user = preprocess(\"zineb\")\n",
    "X = dataframe_user[dataframe_user.columns.difference(\n",
    "    ['user', 'pressed', 'released', 'key'])]\n",
    "\n",
    "knn_result = loaded_knn.predict(loaded_scaler.transform(X))\n",
    "certainty = max(np.bincount(knn_result)) * 100 / sum(np.bincount(knn_result))\n",
    "print(certainty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 6))\n",
    "ax = fig.add_axes([0, 0, 1, 1])\n",
    "ax.barh(registered_users, list(np.bincount(knn_result)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Gradient Boosting, the accuracy is dramatically increaced, with Zineb's typing being predicted with 93.7% certainty. On the below graph we see and even larger contrast between the users, which makes this model even more capable of predicting the correct user's typing patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_result = loaded_gb.predict(loaded_scaler.transform(X))\n",
    "certainty = max(np.bincount(gb_result)) * 100 / sum(np.bincount(gb_result))\n",
    "print(certainty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 6))\n",
    "ax = fig.add_axes([0,0,1,1])\n",
    "ax.barh(registered_users, list(np.bincount(gb_result)))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
